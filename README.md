# 高并发点赞系统设计

## 1.0
目前使用到的技术栈有springboot3 + mybatis-plus + 声明式事务 + redisson分布式锁 + mysql + knife4j

基础实现用户、点赞、博客等功能

## 2.0
1.引入 spring-session 可以实现分布式登录态，存30分钟

2.利用 jedis + spring-boot-starter-data-redis 实现登录态的持久化存储、使用redis的hash结构替换点赞判断逻辑，减轻数据库压力

### 冷热数据分离拓展：
1.定义存储结构

2.设置值，点赞记录id加上过期时间戳

3.判断是否为blog 的创建时间是否在“一个月前”这个时间点之后。 查询hash结构的数据，不是的话，值为null就代表没有点过赞， 使用instanceof结构类型，比较当前时间和过期，如果大于就使用jdk21特性虚拟线程Thread.ofVirtual().start(执行删除hash操作) 返回false;没有过期并且又是存在的, 返回true

4.不在一个月内，就是mybatis-plus的exist查询返回结果

## 3.0
为了提升性能，使用Redis临时点赞数据，加定时任务落库 + 补偿操作

1.引入了lua脚本替代掉同步锁和事务,复杂逻辑迁移

2.新编写了redis实现的点赞服务，通过不同的服务名称注入

3.新增了定时任务，按顺序执行 点赞(新增记录)，再取消点赞(移除记录)，博客点赞数(更新)

4.使用虚拟线程异步删除这个时间片内的所有redis数据

5.异步化定时任务操作，解决任务阻塞的问题

## 4.0
为了解决以下两个问题： 单点热点问题和恶意用户刷接口

1.引入caffeine来构建多级缓存框架，进一步提升性能, 
多级缓存框架：(L1)Caffeine + (L2)Redis + (L3)MySQL.

2.根据网上的例子实现HeavyKeeper算法, 来检测热点Key，为热点数据，就缓存到本地数据中提高性能

## 5.0 
虽然已经基于Redis、Caffeine和定时任务构建了高性能的点赞系统。

但是目前存在问题以下几个问题：

1.点赞操作与后续的数据处理(如计数更新、消息通知)强耦合，不利于功能扩展。

2.Redis和数据库之间数据同步依赖定时任务，缺乏实时性和可靠性保障。

引入消息队列pulsar，对系统进行异步化构造处理，提高系统削峰填谷能力和系统可用性。

具体设计分为三个部分：

1.用户进行点赞/取消点赞，执行lua脚本判断点赞状态，返回失败则不需要继续处理，成功就构造点赞事件发送到消息队列中，
如果发送成功就返回true，否则回滚Redis的状态，保证数据一致性。

2.下游消费者消息处理，批量消费1000条/10秒，数据库持久化，如果成功了，就自动ack消息，失败了进行3次重试，
重试还是失败，就放入死信队列进行响应的逻辑处理

3.最终一致性保障(对账机制)，分别从redis和mysql取出数据，借助guava库的Sets.difference方法获得差异的博客数据，
发送补偿消息事件

## 6.0
引入分布式数据库TiDB，提高数据库的水平扩展能力

随着业务规模持续扩大，点赞数据越来越多，我们会发现当前系统面临着几个关键挑战：

1.传统mysql单机部署的存储容量和性能已经难以满足不断增长的用户和内容需求，而数据库分库分表
又有较高的研发和运维成本。

2.业务中经常需要做跨表查询和修改，如点赞后内容计数表，分库分表后事务一致性难以保证。

3.数据库扩容、分片再平衡、备份恢复等运维工作复杂，系统的水平扩展能力也受到严重限制。

由TiDB Server、PD、存储节点(TiKV Server和 TiFlash)三个部分组成：

TiDB Server：负责SQL解析和请求处理

PDServer：确保调度中心高可用

TiKV Server：数据分片存储

TiFlash：用于分析查询加速

生产环境中遇到数据迁移的需求，一定要由完善的流程和策略，举个例子：

1.全量数据同步：使用TiDB DM工具进行初始数据迁移

2.增量数据同步：配置MySQL Binlog到TiDB的实时同步

3.双写验证阶段：应用同时写入MySQL和TiDB, 比对数据一致性

4.切换读流量：将请求切换到TiDB中

5.切换写流量：确认无问题，将写请求切换到TiDB中

6.下线旧MySQL：完成数据迁移后，逐步下线旧的MySQL实例

## 6.0
引入分布式数据库TiDB，提高数据库的水平扩展能力

随着业务规模持续扩大，点赞数据越来越多，我们会发现当前系统面临着几个关键挑战：

1.传统mysql单机部署的存储容量和性能已经难以满足不断增长的用户和内容需求，而数据库分库分表
又有较高的研发和运维成本。

2.业务中经常需要做跨表查询和修改，如点赞后内容计数表，分库分表后事务一致性难以保证。

3.数据库扩容、分片再平衡、备份恢复等运维工作复杂，系统的水平扩展能力也受到严重限制。

由TiDB Server、PD、存储节点(TiKV Server和 TiFlash)三个部分组成：

TiDB Server：负责SQL解析和请求处理

PDServer：确保调度中心高可用

TiKV Server：数据分片存储

TiFlash：用于分析查询加速

生产环境中遇到数据迁移的需求，一定要由完善的流程和策略，举个例子：

1.全量数据同步：使用TiDB DM工具进行初始数据迁移

2.增量数据同步：配置MySQL Binlog到TiDB的实时同步

3.双写验证阶段：应用同时写入MySQL和TiDB, 比对数据一致性

4.切换读流量：将请求切换到TiDB中

5.切换写流量：确认无问题，将写请求切换到TiDB中

6.下线旧MySQL：完成数据迁移后，逐步下线旧的MySQL实例

## 压测

受限于电脑配置太差

2.0：tps 319

3.0：tps 理想中(3k-4k左右)，可实际上只有 650

5.0：tps 理想中(2.0十倍以上)，可实际上还不如2.0 (削峰填谷)

使用Hutool工具类的StopWatch看耗时日志发现，redis比pulsar快，是因为发送消息消耗的时间
慢太多了

在真实的场景下，还会受到网络io的影响，按道理来说，3.0和5.0不会相差太多的

其实还能再凹一下更高的TPS

可以去配置tomcat服务的最大线程数、Redis、mysql等连接数、pulsar批量处理数